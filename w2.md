**核心架构任务：搭建消息队列与数据流改造 (3天)**
*   **目标：** 建立基于Redis消息队列的异步、解耦数据处理管道。
*   **具体步骤：**
    1.  **Redis安装与配置：** 确保本地或Docker环境中Redis服务运行正常。
    2.  **Python环境RQ集成：** 在Python项目的`requirements.txt`中添加`redis`和`rq`库，并安装。
    3.  **`news_crawler.py`改造为生产者：**
        *   引入`redis`和`rq`库，创建Redis连接和RQ队列实例。
        *   修改`crawl_news`函数，使其专注于从**新浪财经**抓取新闻标题、URL、发布时间及新闻详情内容。
        *   在抓取到新闻数据后，将新闻内容（或包含新闻URL和ID的字典）封装为任务，使用`queue.enqueue(worker_function, news_data)`将其放入Redis队列。
        *   记录任务入队成功日志。
    4.  **创建`worker.py`消费者：**
        *   新建`worker.py`文件。
        *   引入`redis`, `rq`, `analyzer`模块和数据库操作模块。
        *   定义一个处理函数（例如`process_news_task(news_data)`），该函数将接收来自队列的任务数据。
        *   在`process_news_task`中，调用`CostAwareAnalyzer`对新闻进行分析。
        *   将分析结果（摘要、事件、情绪得分等）存入数据库。
        *   编写`main`函数，启动一个RQ Worker实例，监听指定队列。
        *   添加日志记录，显示任务消费、分析和存储的状态。
    5.  **验证消息队列：** 运行`news_crawler.py`，观察Redis队列中任务的增加。在另一个终端启动`worker.py`，观察任务被消费和处理的过程。

